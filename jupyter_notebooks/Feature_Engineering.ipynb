{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **(ADD THE NOTEBOOK NAME HERE)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Write your notebook objective here, for example, \"Fetch data from Kaggle and save as raw data\", or \"engineer features for modelling\"\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* Write here which data or information you need to run the notebook \n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Write here which files, code or artefacts you generate by the end of the notebook \n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* In case you have any additional comments that don't fit in the previous bullets, please state them here. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load Cleaned Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Section 1 content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train_set_path = \"outputs/datasets/cleaned/TrainSetCleaned.csv\"\n",
        "TrainSet = pd.read_csv(train_set_path)\n",
        "TrainSet.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_set_path = 'outputs/datasets/cleaned/TestSetCleaned.csv'\n",
        "TestSet = pd.read_csv(test_set_path)\n",
        "TestSet.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ppscore as pps\n",
        "%matplotlib inline\n",
        "\n",
        "def heatmap_corr(df, threshold, figsize=(20, 12), font_annot=8):\n",
        "    if len(df.columns) > 1:\n",
        "        mask = np.zeros_like(df, dtype=np.bool)\n",
        "        mask[np.triu_indices_from(mask)] = True\n",
        "        mask[abs(df) < threshold] = True\n",
        "\n",
        "        fig, axes = plt.subplots(figsize=figsize)\n",
        "        sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
        "                    mask=mask, cmap='viridis', annot_kws={\"size\": font_annot}, ax=axes,\n",
        "                    linewidth=0.5\n",
        "                    )\n",
        "        axes.set_yticklabels(df.columns, rotation=0)\n",
        "        plt.ylim(len(df.columns), 0)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def heatmap_pps(df, threshold, figsize=(20, 12), font_annot=8):\n",
        "    if len(df.columns) > 1:\n",
        "        mask = np.zeros_like(df, dtype=np.bool)\n",
        "        mask[abs(df) < threshold] = True\n",
        "        fig, ax = plt.subplots(figsize=figsize)\n",
        "        ax = sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
        "                         mask=mask, cmap='rocket_r', annot_kws={\"size\": font_annot},\n",
        "                         linewidth=0.05, linecolor='grey')\n",
        "        plt.ylim(len(df.columns), 0)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def CalculateCorrAndPPS(df):\n",
        "    df_corr_spearman = df.corr(method=\"spearman\")\n",
        "    df_corr_pearson = df.corr(method=\"pearson\")\n",
        "\n",
        "    pps_matrix_raw = pps.matrix(df)\n",
        "    pps_matrix = pps_matrix_raw.filter(['x', 'y', 'ppscore']).pivot(columns='x', index='y', values='ppscore')\n",
        "\n",
        "    pps_score_stats = pps_matrix_raw.query(\"ppscore < 1\").filter(['ppscore']).describe().T\n",
        "    print(\"PPS threshold - check PPS score IQR to decide threshold for heatmap \\n\")\n",
        "    print(pps_score_stats.round(3))\n",
        "\n",
        "    return df_corr_pearson, df_corr_spearman, pps_matrix\n",
        "\n",
        "\n",
        "def DisplayCorrAndPPS(df_corr_pearson, df_corr_spearman, pps_matrix, CorrThreshold, PPS_Threshold,\n",
        "                      figsize=(20, 12), font_annot=8):\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"* Analyse how the target variable for your ML models are correlated with other variables (features and target)\")\n",
        "    print(\"* Analyse multi-colinearity, that is, how the features are correlated among themselves\")\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"*** Heatmap: Spearman Correlation ***\")\n",
        "    print(\"It evaluates monotonic relationship \\n\")\n",
        "    heatmap_corr(df=df_corr_spearman, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"*** Heatmap: Pearson Correlation ***\")\n",
        "    print(\"It evaluates the linear relationship between two continuous variables \\n\")\n",
        "    heatmap_corr(df=df_corr_pearson, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"*** Heatmap: Power Predictive Score (PPS) ***\")\n",
        "    print(f\"PPS detects linear or non-linear relationships between two columns.\\n\"\n",
        "          f\"The score ranges from 0 (no predictive power) to 1 (perfect predictive power) \\n\")\n",
        "    heatmap_pps(df=pps_matrix, threshold=PPS_Threshold, figsize=figsize, font_annot=font_annot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_corr_pearson, df_corr_spearman, pps_matrix = CalculateCorrAndPPS(TrainSet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DisplayCorrAndPPS(df_corr_pearson=df_corr_pearson,\n",
        "                  df_corr_spearman=df_corr_spearman, \n",
        "                  pps_matrix=pps_matrix,\n",
        "                  CorrThreshold=0.6, PPS_Threshold=0.15,\n",
        "                  figsize=(10,10), font_annot=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from feature_engine import transformation as vt\n",
        "from feature_engine.outliers import Winsorizer\n",
        "from feature_engine.encoding import OrdinalEncoder\n",
        "sns.set(style=\"whitegrid\")\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "def FeatureEngineeringAnalysis(df, analysis_type=None):\n",
        "    \"\"\"\n",
        "    - used for quick feature engineering on numerical and categorical variables\n",
        "    to decide which transformation can better transform the distribution shape\n",
        "    - Once transformed, use a reporting tool, like pandas-profiling, to evaluate distributions\n",
        "    \"\"\"\n",
        "    check_missing_values(df)\n",
        "    allowed_types = ['numerical', 'ordinal_encoder', 'outlier_winsorizer']\n",
        "    check_user_entry_on_analysis_type(analysis_type, allowed_types)\n",
        "    list_column_transformers = define_list_column_transformers(analysis_type)\n",
        "\n",
        "    # Loop in each variable and engineer the data according to the analysis type\n",
        "    df_feat_eng = pd.DataFrame([])\n",
        "    for column in df.columns:\n",
        "        # create additional columns (column_method) to apply the methods\n",
        "        df_feat_eng = pd.concat([df_feat_eng, df[column]], axis=1)\n",
        "        for method in list_column_transformers:\n",
        "            df_feat_eng[f\"{column}_{method}\"] = df[column]\n",
        "\n",
        "        # Apply transformers in respective column_transformers\n",
        "        df_feat_eng, list_applied_transformers = apply_transformers(\n",
        "            analysis_type, df_feat_eng, column)\n",
        "\n",
        "        # For each variable, assess how the transformations perform\n",
        "        transformer_evaluation(\n",
        "            column, list_applied_transformers, analysis_type, df_feat_eng)\n",
        "\n",
        "    return df_feat_eng\n",
        "\n",
        "\n",
        "def check_user_entry_on_analysis_type(analysis_type, allowed_types):\n",
        "    \"\"\" Check analysis type \"\"\"\n",
        "    if analysis_type is None:\n",
        "        raise SystemExit(\n",
        "            f\"You should pass analysis_type parameter as one of the following options: {allowed_types}\")\n",
        "    if analysis_type not in allowed_types:\n",
        "        raise SystemExit(\n",
        "            f\"analysis_type argument should be one of these options: {allowed_types}\")\n",
        "\n",
        "\n",
        "def check_missing_values(df):\n",
        "    if df.isna().sum().sum() != 0:\n",
        "        raise SystemExit(\n",
        "            f\"There is a missing value in your dataset. Please handle that before getting into feature engineering.\")\n",
        "\n",
        "\n",
        "def define_list_column_transformers(analysis_type):\n",
        "    \"\"\" Set suffix columns according to analysis_type\"\"\"\n",
        "    if analysis_type == 'numerical':\n",
        "        list_column_transformers = [\n",
        "            \"log_e\", \"log_10\", \"reciprocal\", \"power\", \"box_cox\", \"yeo_johnson\"]\n",
        "\n",
        "    elif analysis_type == 'ordinal_encoder':\n",
        "        list_column_transformers = [\"ordinal_encoder\"]\n",
        "\n",
        "    elif analysis_type == 'outlier_winsorizer':\n",
        "        list_column_transformers = ['iqr']\n",
        "\n",
        "    return list_column_transformers\n",
        "\n",
        "\n",
        "def apply_transformers(analysis_type, df_feat_eng, column):\n",
        "    for col in df_feat_eng.select_dtypes(include='category').columns:\n",
        "        df_feat_eng[col] = df_feat_eng[col].astype('object')\n",
        "\n",
        "    if analysis_type == 'numerical':\n",
        "        df_feat_eng, list_applied_transformers = FeatEngineering_Numerical(\n",
        "            df_feat_eng, column)\n",
        "\n",
        "    elif analysis_type == 'outlier_winsorizer':\n",
        "        df_feat_eng, list_applied_transformers = FeatEngineering_OutlierWinsorizer(\n",
        "            df_feat_eng, column)\n",
        "\n",
        "    elif analysis_type == 'ordinal_encoder':\n",
        "        df_feat_eng, list_applied_transformers = FeatEngineering_CategoricalEncoder(\n",
        "            df_feat_eng, column)\n",
        "\n",
        "    return df_feat_eng, list_applied_transformers\n",
        "\n",
        "\n",
        "def transformer_evaluation(column, list_applied_transformers, analysis_type, df_feat_eng):\n",
        "    # For each variable, assess how the transformations perform\n",
        "    print(f\"* Variable Analyzed: {column}\")\n",
        "    print(f\"* Applied transformation: {list_applied_transformers} \\n\")\n",
        "    for col in [column] + list_applied_transformers:\n",
        "\n",
        "        if analysis_type != 'ordinal_encoder':\n",
        "            DiagnosticPlots_Numerical(df_feat_eng, col)\n",
        "\n",
        "        else:\n",
        "            if col == column:\n",
        "                DiagnosticPlots_Categories(df_feat_eng, col)\n",
        "            else:\n",
        "                DiagnosticPlots_Numerical(df_feat_eng, col)\n",
        "\n",
        "        print(\"\\n\")\n",
        "\n",
        "\n",
        "def DiagnosticPlots_Categories(df_feat_eng, col):\n",
        "    plt.figure(figsize=(4, 3))\n",
        "    sns.countplot(data=df_feat_eng, x=col, palette=[\n",
        "                  '#432371'], order=df_feat_eng[col].value_counts().index)\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.suptitle(f\"{col}\", fontsize=30, y=1.05)\n",
        "    plt.show()\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "def DiagnosticPlots_Numerical(df, variable):\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "    sns.histplot(data=df, x=variable, kde=True, element=\"step\", ax=axes[0])\n",
        "    stats.probplot(df[variable], dist=\"norm\", plot=axes[1])\n",
        "    sns.boxplot(x=df[variable], ax=axes[2])\n",
        "\n",
        "    axes[0].set_title('Histogram')\n",
        "    axes[1].set_title('QQ Plot')\n",
        "    axes[2].set_title('Boxplot')\n",
        "    fig.suptitle(f\"{variable}\", fontsize=30, y=1.05)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def FeatEngineering_CategoricalEncoder(df_feat_eng, column):\n",
        "    list_methods_worked = []\n",
        "    try:\n",
        "        encoder = OrdinalEncoder(encoding_method='arbitrary', variables=[\n",
        "                                 f\"{column}_ordinal_encoder\"])\n",
        "        df_feat_eng = encoder.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_ordinal_encoder\")\n",
        "\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_ordinal_encoder\"], axis=1, inplace=True)\n",
        "\n",
        "    return df_feat_eng, list_methods_worked\n",
        "\n",
        "\n",
        "def FeatEngineering_OutlierWinsorizer(df_feat_eng, column):\n",
        "    list_methods_worked = []\n",
        "\n",
        "    # Winsorizer iqr\n",
        "    try:\n",
        "        disc = Winsorizer(\n",
        "            capping_method='iqr', tail='both', fold=1.5, variables=[f\"{column}_iqr\"])\n",
        "        df_feat_eng = disc.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_iqr\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_iqr\"], axis=1, inplace=True)\n",
        "\n",
        "    return df_feat_eng, list_methods_worked\n",
        "\n",
        "\n",
        "def FeatEngineering_Numerical(df_feat_eng, column):\n",
        "    list_methods_worked = []\n",
        "\n",
        "    # LogTransformer base e\n",
        "    try:\n",
        "        lt = vt.LogTransformer(variables=[f\"{column}_log_e\"])\n",
        "        df_feat_eng = lt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_log_e\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_log_e\"], axis=1, inplace=True)\n",
        "\n",
        "    # LogTransformer base 10\n",
        "    try:\n",
        "        lt = vt.LogTransformer(variables=[f\"{column}_log_10\"], base='10')\n",
        "        df_feat_eng = lt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_log_10\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_log_10\"], axis=1, inplace=True)\n",
        "\n",
        "    # ReciprocalTransformer\n",
        "    try:\n",
        "        rt = vt.ReciprocalTransformer(variables=[f\"{column}_reciprocal\"])\n",
        "        df_feat_eng = rt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_reciprocal\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_reciprocal\"], axis=1, inplace=True)\n",
        "\n",
        "    # PowerTransformer\n",
        "    try:\n",
        "        pt = vt.PowerTransformer(variables=[f\"{column}_power\"])\n",
        "        df_feat_eng = pt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_power\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_power\"], axis=1, inplace=True)\n",
        "\n",
        "    # BoxCoxTransformer\n",
        "    try:\n",
        "        bct = vt.BoxCoxTransformer(variables=[f\"{column}_box_cox\"])\n",
        "        df_feat_eng = bct.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_box_cox\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_box_cox\"], axis=1, inplace=True)\n",
        "\n",
        "    # YeoJohnsonTransformer\n",
        "    try:\n",
        "        yjt = vt.YeoJohnsonTransformer(variables=[f\"{column}_yeo_johnson\"])\n",
        "        df_feat_eng = yjt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_yeo_johnson\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_yeo_johnson\"], axis=1, inplace=True)\n",
        "\n",
        "    return df_feat_eng, list_methods_worked"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Encoding"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|Numerical Transformation| Smart Correlation Selection|OrdinalCategoricalEncoder|OutlierTrimmer|\n",
        "|---|---|---|---| \n",
        "|All varibles not salesprice| All Variables apart from sales price|BsmtExposure  |  GrLivArea|\n",
        " |    |                                      | BsmtFinTypel|SalePrice|\n",
        "|     |                                      |GarageFinish|\n",
        "  |   |                                               |KitchenQual|  \n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Categorical Enconding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "variables_engineering= ['BsmtExposure', 'BsmtFinType1', 'GarageFinish', 'KitchenQual']\n",
        "variables_engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineering = TrainSet[variables_engineering].copy()\n",
        "df_engineering.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineering = FeatureEngineeringAnalysis(df=df_engineering, analysis_type='ordinal_encoder')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoder = OrdinalEncoder(encoding_method='arbitrary', variables = variables_engineering)\n",
        "TrainSet = encoder.fit_transform(TrainSet)\n",
        "TestSet = encoder.transform(TestSet)\n",
        "\n",
        "print(\"* Categorical encoding - ordinal transformation done!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Testing Set\")\n",
        "TestSet.filter(variables_engineering).head(3)\n",
        "\n",
        "print(\"Training Set\")\n",
        "TrainSet.filter(variables_engineering).head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "variables_engineering = (TrainSet.select_dtypes(include=['float', 'int']).columns.to_list())\n",
        "variables_engineering.remove('SalePrice')\n",
        "variables_engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "df_engineering = TrainSet[variables_engineering].copy()\n",
        "df_engineering.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineering = FeatureEngineeringAnalysis(df=df_engineering, analysis_type='numerical')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Power        | log e       | Yeo Johnson    | Cox Box |\n",
        "|---|---|---|---|\n",
        "|2ndFlrSF      |1stFlrSF     |1stFlrSF       | 1stFlrSF |\n",
        "|BsmtExposure  |LotFrontage  |GarageArea      | GrLivArea\n",
        "|BsmtFinSF1    | LotArea     |GrLivArea\n",
        "|BsmtUnfSF     | GrLivArea\n",
        "|TotalBsmtSF   |\n",
        "| GarageArea\n",
        "| TotalBsmtSF\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The rest of the varibles showed little or no imporvment in the transfromers. where multipul transformers have been selected, we will use the most simple method. Log(e) is the most simple to preform so it will be selected over the others where possible. Power will also be selected over Yeo Johnson\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "pipeline = Pipeline ([\n",
        "        (\"LogTransformer\", vt.LogTransformer(\n",
        "                         variables = ['1stFlrSF', 'GrLivArea', 'LotArea', 'LotFrontage'])),\n",
        "\n",
        "        (\"PowerTransformer\", vt.PowerTransformer(\n",
        "                         variables = ['2ndFlrSF', 'BsmtExposure', 'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF', 'GarageArea' ])),\n",
        "\n",
        "])\n",
        "\n",
        "train_set = pipeline.fit_transform(TrainSet)\n",
        "test_set = pipeline.transform(TestSet)\n",
        "\n",
        "print(\"Numerical transformation done!\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Outlier Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.outliers import OutlierTrimmer\n",
        "\n",
        "variables_engineering = ['SalePrice', 'GrLivArea']\n",
        "variables_engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineering = TrainSet[variables_engineering].copy()\n",
        "df_engineering.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_transformed = OutlierTrimmer(capping_method='iqr', tail='both', fold=1.5, variables=variables_engineering)\n",
        "df_transformed = df_transformed.fit_transform(df_engineering)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_histogram_and_boxplot(df):\n",
        "  for col in df.columns:\n",
        "    fig, axes = plt.subplots(nrows=2 ,ncols=1 ,figsize=(7,7), gridspec_kw={\"height_ratios\": (.15, .85)})\n",
        "    sns.boxplot(data=df, x=col, ax=axes[0])\n",
        "    sns.histplot(data=df, x=col, kde=True, ax=axes[1])\n",
        "    fig.suptitle(f\"{col} Distribution - Boxplot and Histogram\")\n",
        "    plt.show()\n",
        "\n",
        "    IQR = df[col].quantile(q=0.75) - df[col].quantile(q=0.25)\n",
        "    print(\n",
        "        f\"This is the range where a datapoint is not an outlier: from \"\n",
        "        f\"{(df[col].quantile(q=0.25) - 1.5*IQR).round(2)} to \"\n",
        "        f\"{(df[col].quantile(q=0.75) + 1.5*IQR).round(2)}\")\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"========= Before Transformation ========= \\n\")\n",
        "plot_histogram_and_boxplot(TrainSet[['SalePrice', 'GrLivArea']])\n",
        "print(\"\\n\\n ========= After Transformation =========\")\n",
        "plot_histogram_and_boxplot(df=df_transformed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"* The dataset has {len(TrainSet)} rows, considering outliers.\")\n",
        "\n",
        "transformer = OutlierTrimmer(\n",
        "    variables=['SalePrice', 'GrLivArea'],\n",
        "    tail = 'both',\n",
        "    capping_method='iqr',\n",
        "    fold=1.5\n",
        ")\n",
        "\n",
        "transformer.fit(TrainSet)\n",
        "\n",
        "TrainSet, TestSet = transformer.transform(TrainSet) , transformer.transform(TestSet)\n",
        "\n",
        "\n",
        "print(f\"* Once it is transformed with OutlierTrimmer, dataset has {len(TrainSet)} rows\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Winsorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.outliers import Winsorizer\n",
        "\n",
        "variables_engineering = ['SalePrice', 'GrLivArea']\n",
        "variables_engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineering = TrainSet[variables_engineering].copy()\n",
        "df_engineering.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineering = FeatureEngineeringAnalysis(df=df_engineering, analysis_type='outlier_winsorizer')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transformer = Winsorizer(\n",
        "    capping_method='iqr', \n",
        "    tail='both', \n",
        "    fold=1.5, \n",
        "    variables=variables_engineering)\n",
        "transformer.fit(TrainSet)\n",
        "\n",
        "TrainSet, TestSet = transformer.transform(TrainSet) , transformer.transform(TestSet)\n",
        "\n",
        "print(\"* Winsorizer transformation done!\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Smart Correlation Selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineering = TrainSet.copy()\n",
        "df_engineering.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.selection import SmartCorrelatedSelection\n",
        "corr_sel = SmartCorrelatedSelection(variables=None, method=\"spearman\", threshold=0.8,selection_method=\"variance\", missing_values='ignore')\n",
        "\n",
        "corr_sel.fit_transform(df_engineering)\n",
        "corr_sel.correlated_feature_sets_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corr_sel.features_to_drop_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Section 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Section 2 content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* You may add as many sections as you want, as long as they support your project workflow.\n",
        "* All notebook's cells should be run top-down (you can't create a dynamic wherein a given point you need to go back to a previous cell to execute some task, like go back to a previous cell and refresh a variable content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Push files to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* If you do not need to push files to Repo, you may replace this section with \"Conclusions and Next Steps\" and state your conclusions and next steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKlnIozA4eQO",
        "outputId": "fd09bc1f-adb1-4511-f6ce-492a6af570c0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "  # create here your folder\n",
        "  # os.makedirs(name='')\n",
        "except Exception as e:\n",
        "  print(e)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12 (default, Feb 16 2023, 12:40:47) \n[GCC 9.4.0]"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
