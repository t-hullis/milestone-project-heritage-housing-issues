{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Feature Engineering**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "Engineer features for Classification, Regression and Cluster models\n",
        "\n",
        "## Inputs\n",
        "\n",
        "inputs/datasets/cleaned/TrainSet.csv\n",
        "inputs/datasets/cleaned/TestSet.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "Create a list with variables to engineer\n",
        "\n",
        "## Conclusions\n",
        "\n",
        " LogTransformer : ['1stFlrSF', 'GrLivArea', 'LotArea', 'LotFrontage'])),\n",
        "\n",
        "PowerTransformer : ('BsmtFinSF1', 'BsmtUnfSF', 'GarageArea', 'GrLivArea', 'MasVnrArea', 'OpenPorchSF' ]\n",
        "\n",
        "YeoJohnsonTransformer : ['1stFlrSF', 'BsmtUnfSF', 'GarageArea', 'GrLivArea', 'OpenPorchSF', 'TotalBsmtSF']\n",
        "\n",
        "Winsorizer : ['GrLivArea'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load Cleaned Data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load cleaned data in to pandas data frame"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train_set_path = \"outputs/datasets/cleaned/TrainSetCleaned.csv\"\n",
        "TrainSet = pd.read_csv(train_set_path)\n",
        "TrainSet.head(3)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "train set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_set_path = 'outputs/datasets/cleaned/TestSetCleaned.csv'\n",
        "TestSet = pd.read_csv(test_set_path)\n",
        "TestSet.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Exploration"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Explore potential transfromations for our dataset\n",
        "\n",
        "Use Correlation and PPS Studies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ppscore as pps\n",
        "%matplotlib inline\n",
        "\n",
        "def heatmap_corr(df, threshold, figsize=(20, 12), font_annot=8):\n",
        "    if len(df.columns) > 1:\n",
        "        mask = np.zeros_like(df, dtype=np.bool)\n",
        "        mask[np.triu_indices_from(mask)] = True\n",
        "        mask[abs(df) < threshold] = True\n",
        "\n",
        "        fig, axes = plt.subplots(figsize=figsize)\n",
        "        sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
        "                    mask=mask, cmap='viridis', annot_kws={\"size\": font_annot}, ax=axes,\n",
        "                    linewidth=0.5\n",
        "                    )\n",
        "        axes.set_yticklabels(df.columns, rotation=0)\n",
        "        plt.ylim(len(df.columns), 0)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def heatmap_pps(df, threshold, figsize=(20, 12), font_annot=8):\n",
        "    if len(df.columns) > 1:\n",
        "        mask = np.zeros_like(df, dtype=np.bool)\n",
        "        mask[abs(df) < threshold] = True\n",
        "        fig, ax = plt.subplots(figsize=figsize)\n",
        "        ax = sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
        "                         mask=mask, cmap='rocket_r', annot_kws={\"size\": font_annot},\n",
        "                         linewidth=0.05, linecolor='grey')\n",
        "        plt.ylim(len(df.columns), 0)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def CalculateCorrAndPPS(df):\n",
        "    df_corr_spearman = df.corr(method=\"spearman\")\n",
        "    df_corr_pearson = df.corr(method=\"pearson\")\n",
        "\n",
        "    pps_matrix_raw = pps.matrix(df)\n",
        "    pps_matrix = pps_matrix_raw.filter(['x', 'y', 'ppscore']).pivot(columns='x', index='y', values='ppscore')\n",
        "\n",
        "    pps_score_stats = pps_matrix_raw.query(\"ppscore < 1\").filter(['ppscore']).describe().T\n",
        "    print(\"PPS threshold - check PPS score IQR to decide threshold for heatmap \\n\")\n",
        "    print(pps_score_stats.round(3))\n",
        "\n",
        "    return df_corr_pearson, df_corr_spearman, pps_matrix\n",
        "\n",
        "\n",
        "def DisplayCorrAndPPS(df_corr_pearson, df_corr_spearman, pps_matrix, CorrThreshold, PPS_Threshold,\n",
        "                      figsize=(20, 12), font_annot=8):\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"* Analyse how the target variable for your ML models are correlated with other variables (features and target)\")\n",
        "    print(\"* Analyse multi-colinearity, that is, how the features are correlated among themselves\")\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"*** Heatmap: Spearman Correlation ***\")\n",
        "    print(\"It evaluates monotonic relationship \\n\")\n",
        "    heatmap_corr(df=df_corr_spearman, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"*** Heatmap: Pearson Correlation ***\")\n",
        "    print(\"It evaluates the linear relationship between two continuous variables \\n\")\n",
        "    heatmap_corr(df=df_corr_pearson, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"*** Heatmap: Power Predictive Score (PPS) ***\")\n",
        "    print(f\"PPS detects linear or non-linear relationships between two columns.\\n\"\n",
        "          f\"The score ranges from 0 (no predictive power) to 1 (perfect predictive power) \\n\")\n",
        "    heatmap_pps(df=pps_matrix, threshold=PPS_Threshold, figsize=figsize, font_annot=font_annot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_corr_pearson, df_corr_spearman, pps_matrix = CalculateCorrAndPPS(TrainSet)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Show heatmaps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DisplayCorrAndPPS(df_corr_pearson=df_corr_pearson,\n",
        "                  df_corr_spearman=df_corr_spearman, \n",
        "                  pps_matrix=pps_matrix,\n",
        "                  CorrThreshold=0.6, PPS_Threshold=0.15,\n",
        "                  figsize=(10,10), font_annot=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use custom function from Code Institute\n",
        "\n",
        "Use this function to help decide the best tranfromations to make on each variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from feature_engine import transformation as vt\n",
        "from feature_engine.outliers import Winsorizer\n",
        "from feature_engine.encoding import OrdinalEncoder\n",
        "sns.set(style=\"whitegrid\")\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "def FeatureEngineeringAnalysis(df, analysis_type=None):\n",
        "    \"\"\"\n",
        "    - used for quick feature engineering on numerical and categorical variables\n",
        "    to decide which transformation can better transform the distribution shape\n",
        "    - Once transformed, use a reporting tool, like pandas-profiling, to evaluate distributions\n",
        "    \"\"\"\n",
        "    check_missing_values(df)\n",
        "    allowed_types = ['numerical', 'ordinal_encoder', 'outlier_winsorizer']\n",
        "    check_user_entry_on_analysis_type(analysis_type, allowed_types)\n",
        "    list_column_transformers = define_list_column_transformers(analysis_type)\n",
        "\n",
        "    # Loop in each variable and engineer the data according to the analysis type\n",
        "    df_feat_eng = pd.DataFrame([])\n",
        "    for column in df.columns:\n",
        "        # create additional columns (column_method) to apply the methods\n",
        "        df_feat_eng = pd.concat([df_feat_eng, df[column]], axis=1)\n",
        "        for method in list_column_transformers:\n",
        "            df_feat_eng[f\"{column}_{method}\"] = df[column]\n",
        "\n",
        "        # Apply transformers in respective column_transformers\n",
        "        df_feat_eng, list_applied_transformers = apply_transformers(\n",
        "            analysis_type, df_feat_eng, column)\n",
        "\n",
        "        # For each variable, assess how the transformations perform\n",
        "        transformer_evaluation(\n",
        "            column, list_applied_transformers, analysis_type, df_feat_eng)\n",
        "\n",
        "    return df_feat_eng\n",
        "\n",
        "\n",
        "def check_user_entry_on_analysis_type(analysis_type, allowed_types):\n",
        "    \"\"\" Check analysis type \"\"\"\n",
        "    if analysis_type is None:\n",
        "        raise SystemExit(\n",
        "            f\"You should pass analysis_type parameter as one of the following options: {allowed_types}\")\n",
        "    if analysis_type not in allowed_types:\n",
        "        raise SystemExit(\n",
        "            f\"analysis_type argument should be one of these options: {allowed_types}\")\n",
        "\n",
        "\n",
        "def check_missing_values(df):\n",
        "    if df.isna().sum().sum() != 0:\n",
        "        raise SystemExit(\n",
        "            f\"There is a missing value in your dataset. Please handle that before getting into feature engineering.\")\n",
        "\n",
        "\n",
        "def define_list_column_transformers(analysis_type):\n",
        "    \"\"\" Set suffix columns according to analysis_type\"\"\"\n",
        "    if analysis_type == 'numerical':\n",
        "        list_column_transformers = [\n",
        "            \"log_e\", \"log_10\", \"reciprocal\", \"power\", \"box_cox\", \"yeo_johnson\"]\n",
        "\n",
        "    elif analysis_type == 'ordinal_encoder':\n",
        "        list_column_transformers = [\"ordinal_encoder\"]\n",
        "\n",
        "    elif analysis_type == 'outlier_winsorizer':\n",
        "        list_column_transformers = ['iqr']\n",
        "\n",
        "    return list_column_transformers\n",
        "\n",
        "\n",
        "def apply_transformers(analysis_type, df_feat_eng, column):\n",
        "    for col in df_feat_eng.select_dtypes(include='category').columns:\n",
        "        df_feat_eng[col] = df_feat_eng[col].astype('object')\n",
        "\n",
        "    if analysis_type == 'numerical':\n",
        "        df_feat_eng, list_applied_transformers = FeatEngineering_Numerical(\n",
        "            df_feat_eng, column)\n",
        "\n",
        "    elif analysis_type == 'outlier_winsorizer':\n",
        "        df_feat_eng, list_applied_transformers = FeatEngineering_OutlierWinsorizer(\n",
        "            df_feat_eng, column)\n",
        "\n",
        "    elif analysis_type == 'ordinal_encoder':\n",
        "        df_feat_eng, list_applied_transformers = FeatEngineering_CategoricalEncoder(\n",
        "            df_feat_eng, column)\n",
        "\n",
        "    return df_feat_eng, list_applied_transformers\n",
        "\n",
        "\n",
        "def transformer_evaluation(column, list_applied_transformers, analysis_type, df_feat_eng):\n",
        "    # For each variable, assess how the transformations perform\n",
        "    print(f\"* Variable Analyzed: {column}\")\n",
        "    print(f\"* Applied transformation: {list_applied_transformers} \\n\")\n",
        "    for col in [column] + list_applied_transformers:\n",
        "\n",
        "        if analysis_type != 'ordinal_encoder':\n",
        "            DiagnosticPlots_Numerical(df_feat_eng, col)\n",
        "\n",
        "        else:\n",
        "            if col == column:\n",
        "                DiagnosticPlots_Categories(df_feat_eng, col)\n",
        "            else:\n",
        "                DiagnosticPlots_Numerical(df_feat_eng, col)\n",
        "\n",
        "        print(\"\\n\")\n",
        "\n",
        "\n",
        "def DiagnosticPlots_Categories(df_feat_eng, col):\n",
        "    plt.figure(figsize=(4, 3))\n",
        "    sns.countplot(data=df_feat_eng, x=col, palette=[\n",
        "                  '#432371'], order=df_feat_eng[col].value_counts().index)\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.suptitle(f\"{col}\", fontsize=30, y=1.05)\n",
        "    plt.show()\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "def DiagnosticPlots_Numerical(df, variable):\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "    sns.histplot(data=df, x=variable, kde=True, element=\"step\", ax=axes[0])\n",
        "    stats.probplot(df[variable], dist=\"norm\", plot=axes[1])\n",
        "    sns.boxplot(x=df[variable], ax=axes[2])\n",
        "\n",
        "    axes[0].set_title('Histogram')\n",
        "    axes[1].set_title('QQ Plot')\n",
        "    axes[2].set_title('Boxplot')\n",
        "    fig.suptitle(f\"{variable}\", fontsize=30, y=1.05)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def FeatEngineering_CategoricalEncoder(df_feat_eng, column):\n",
        "    list_methods_worked = []\n",
        "    try:\n",
        "        encoder = OrdinalEncoder(encoding_method='arbitrary', variables=[\n",
        "                                 f\"{column}_ordinal_encoder\"])\n",
        "        df_feat_eng = encoder.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_ordinal_encoder\")\n",
        "\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_ordinal_encoder\"], axis=1, inplace=True)\n",
        "\n",
        "    return df_feat_eng, list_methods_worked\n",
        "\n",
        "\n",
        "def FeatEngineering_OutlierWinsorizer(df_feat_eng, column):\n",
        "    list_methods_worked = []\n",
        "\n",
        "    # Winsorizer iqr\n",
        "    try:\n",
        "        disc = Winsorizer(\n",
        "            capping_method='iqr', tail='both', fold=1.5, variables=[f\"{column}_iqr\"])\n",
        "        df_feat_eng = disc.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_iqr\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_iqr\"], axis=1, inplace=True)\n",
        "\n",
        "    return df_feat_eng, list_methods_worked\n",
        "\n",
        "\n",
        "def FeatEngineering_Numerical(df_feat_eng, column):\n",
        "    list_methods_worked = []\n",
        "\n",
        "    # LogTransformer base e\n",
        "    try:\n",
        "        lt = vt.LogTransformer(variables=[f\"{column}_log_e\"])\n",
        "        df_feat_eng = lt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_log_e\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_log_e\"], axis=1, inplace=True)\n",
        "\n",
        "    # LogTransformer base 10\n",
        "    try:\n",
        "        lt = vt.LogTransformer(variables=[f\"{column}_log_10\"], base='10')\n",
        "        df_feat_eng = lt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_log_10\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_log_10\"], axis=1, inplace=True)\n",
        "\n",
        "    # ReciprocalTransformer\n",
        "    try:\n",
        "        rt = vt.ReciprocalTransformer(variables=[f\"{column}_reciprocal\"])\n",
        "        df_feat_eng = rt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_reciprocal\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_reciprocal\"], axis=1, inplace=True)\n",
        "\n",
        "    # PowerTransformer\n",
        "    try:\n",
        "        pt = vt.PowerTransformer(variables=[f\"{column}_power\"])\n",
        "        df_feat_eng = pt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_power\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_power\"], axis=1, inplace=True)\n",
        "\n",
        "    # BoxCoxTransformer\n",
        "    try:\n",
        "        bct = vt.BoxCoxTransformer(variables=[f\"{column}_box_cox\"])\n",
        "        df_feat_eng = bct.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_box_cox\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_box_cox\"], axis=1, inplace=True)\n",
        "\n",
        "    # YeoJohnsonTransformer\n",
        "    try:\n",
        "        yjt = vt.YeoJohnsonTransformer(variables=[f\"{column}_yeo_johnson\"])\n",
        "        df_feat_eng = yjt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_yeo_johnson\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_yeo_johnson\"], axis=1, inplace=True)\n",
        "\n",
        "    return df_feat_eng, list_methods_worked"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Encoding"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This table shows how each var should be handled, in terms of feature engineering"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|Numerical Transformation| Smart Correlation Selection|OrdinalCategoricalEncoder|OutlierTrimmer|\n",
        "|---|---|---|---| \n",
        "|All varibles not salesprice| All Variables apart from sales price|BsmtExposure  |  GrLivArea|\n",
        " |    |                                      | BsmtFinTypel|SalePrice|\n",
        "|     |                                      |GarageFinish|\n",
        "  |   |                                               |KitchenQual|  \n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To do the transformations, follow these steps:\n",
        "\n",
        "Step 1: Select variable(s)\n",
        "Step 2: Create a separate dataframe with your variable(s)\n",
        "Step 3: Create engineered variables(s), apply the transformation(s), assess engineered variables distribution and select the most suitable method for each variable\n",
        "Step 4 - Apply the selected transformation to the Train and Test set"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Categorical Enconding"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To convert categorical data into numerical"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "step one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "variables_engineering= ['BsmtExposure', 'BsmtFinType1', 'GarageFinish', 'KitchenQual']\n",
        "variables_engineering"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step two:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineering = TrainSet[variables_engineering].copy()\n",
        "df_engineering.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "step three:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineering = FeatureEngineeringAnalysis(df=df_engineering, analysis_type='ordinal_encoder')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "step four:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoder = OrdinalEncoder(encoding_method='arbitrary', variables = variables_engineering)\n",
        "TrainSet = encoder.fit_transform(TrainSet)\n",
        "TestSet = encoder.transform(TestSet)\n",
        "\n",
        "print(\"* Categorical encoding - ordinal transformation done!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Testing Set\")\n",
        "TestSet.filter(variables_engineering).head(3)\n",
        "\n",
        "print(\"Training Set\")\n",
        "TrainSet.filter(variables_engineering).head(3)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Numerical"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "variables_engineering = (TrainSet.select_dtypes(include=['float', 'int']).columns.to_list())\n",
        "variables_engineering.remove('SalePrice')\n",
        "variables_engineering"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "step two"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "df_engineering = TrainSet[variables_engineering].copy()\n",
        "df_engineering.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step three"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineering = FeatureEngineeringAnalysis(df=df_engineering, analysis_type='numerical')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Power        | log e       | Yeo Johnson    | Cox Box |\n",
        "|---|---|---|---|\n",
        "| TotalBsmtSF  |1stFlrSF     |1stFlrSF       | 1stFlrSF |\n",
        "|GarageArea    |LotFrontage  |GarageArea      | GrLivArea\n",
        "|BsmtFinSF1    | LotArea     |GrLivArea\n",
        "|BsmtUnfSF     | GrLivArea\n",
        "|TotalBsmtSF   |\n",
        "| GrLivArea\n",
        "\n",
        "'BsmtFinSF1', 'BsmtUnfSF', 'GarageArea', 'GrLivArea', 'MasVnrArea', 'OpenPorchSF'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The rest of the varibles showed little or no imporvment in the transfromers. where multipul transformers have been selected, we will use the most simple method. Log(e) is the most simple to preform so it will be selected over the others where possible. Power will also be selected over Yeo Johnson\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step four"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "pipeline = Pipeline ([\n",
        "        (\"LogTransformer\", vt.LogTransformer(\n",
        "                         variables = ['1stFlrSF', 'GrLivArea', 'LotArea', 'LotFrontage'])),\n",
        "\n",
        "        (\"PowerTransformer\", vt.PowerTransformer(\n",
        "                         variables = ['BsmtFinSF1', 'BsmtUnfSF', 'GarageArea', 'GrLivArea', 'MasVnrArea', 'OpenPorchSF' ])),\n",
        "\n",
        "        (\"YeoJohnsonTransformer\", vt.YeoJohnsonTransformer(\n",
        "                         variables = ['1stFlrSF', 'BsmtUnfSF', 'GarageArea', 'GrLivArea', 'OpenPorchSF', 'TotalBsmtSF'])),\n",
        "\n",
        "        (\"Winsorizer\", Winsorizer(capping_method='iqr', tail='both', fold=1.5,\n",
        "                        variables = ['GrLivArea'])),\n",
        "])\n",
        "\n",
        "train_set = pipeline.fit_transform(TrainSet)\n",
        "test_set = pipeline.transform(TestSet)\n",
        "\n",
        "print(\"Numerical transformation done!\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Outlier Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.outliers import OutlierTrimmer\n",
        "\n",
        "variables_engineering = ['SalePrice', 'GrLivArea']\n",
        "variables_engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineering = TrainSet[variables_engineering].copy()\n",
        "df_engineering.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_transformed = OutlierTrimmer(capping_method='iqr', tail='both', fold=1.5, variables=variables_engineering)\n",
        "df_transformed = df_transformed.fit_transform(df_engineering)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_histogram_and_boxplot(df):\n",
        "  for col in df.columns:\n",
        "    fig, axes = plt.subplots(nrows=2 ,ncols=1 ,figsize=(7,7), gridspec_kw={\"height_ratios\": (.15, .85)})\n",
        "    sns.boxplot(data=df, x=col, ax=axes[0])\n",
        "    sns.histplot(data=df, x=col, kde=True, ax=axes[1])\n",
        "    fig.suptitle(f\"{col} Distribution - Boxplot and Histogram\")\n",
        "    plt.show()\n",
        "\n",
        "    IQR = df[col].quantile(q=0.75) - df[col].quantile(q=0.25)\n",
        "    print(\n",
        "        f\"This is the range where a datapoint is not an outlier: from \"\n",
        "        f\"{(df[col].quantile(q=0.25) - 1.5*IQR).round(2)} to \"\n",
        "        f\"{(df[col].quantile(q=0.75) + 1.5*IQR).round(2)}\")\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"========= Before Transformation ========= \\n\")\n",
        "plot_histogram_and_boxplot(TrainSet[['SalePrice', 'GrLivArea']])\n",
        "print(\"\\n\\n ========= After Transformation =========\")\n",
        "plot_histogram_and_boxplot(df=df_transformed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"* The dataset has {len(TrainSet)} rows, considering outliers.\")\n",
        "\n",
        "transformer = OutlierTrimmer(\n",
        "    variables=['SalePrice', 'GrLivArea'],\n",
        "    tail = 'both',\n",
        "    capping_method='iqr',\n",
        "    fold=1.5\n",
        ")\n",
        "\n",
        "transformer.fit(TrainSet)\n",
        "\n",
        "TrainSet, TestSet = transformer.transform(TrainSet) , transformer.transform(TestSet)\n",
        "\n",
        "\n",
        "print(f\"* Once it is transformed with OutlierTrimmer, dataset has {len(TrainSet)} rows\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Winsorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.outliers import Winsorizer\n",
        "\n",
        "variables_engineering = ['SalePrice', 'GrLivArea']\n",
        "variables_engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineering = TrainSet[variables_engineering].copy()\n",
        "df_engineering.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineering = FeatureEngineeringAnalysis(df=df_engineering, analysis_type='outlier_winsorizer')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transformer = Winsorizer(\n",
        "    capping_method='iqr', \n",
        "    tail='both', \n",
        "    fold=1.5, \n",
        "    variables=variables_engineering)\n",
        "transformer.fit(TrainSet)\n",
        "\n",
        "TrainSet, TestSet = transformer.transform(TrainSet) , transformer.transform(TestSet)\n",
        "\n",
        "print(\"* Winsorizer transformation done!\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Smart Correlation Selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineering = TrainSet.copy()\n",
        "df_engineering.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.selection import SmartCorrelatedSelection\n",
        "corr_sel = SmartCorrelatedSelection(variables=None, method=\"spearman\", threshold=0.8,selection_method=\"variance\", missing_values='ignore')\n",
        "\n",
        "corr_sel.fit_transform(df_engineering)\n",
        "corr_sel.correlated_feature_sets_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corr_sel.features_to_drop_\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Cleaning\n",
        "        (\"MedianImputation : ['BedroomAbvGr' , 'GarageYrBlt', 'LotFrontage'])\n",
        "        \n",
        "        ArbitraryNumberImputer : ['EnclosedPorch','2ndFlrSF', 'WoodDeckSF', 'MasVnrArea']\n",
        "        \n",
        "        CategoricalImputer : ['GarageFinish','BsmtFinType1']\n",
        "\n",
        "    \n",
        "        OrdinalCategoricalEncoder : ['BsmtExposure', 'BsmtFinType1', 'GarageFinish', 'KitchenQual']\n",
        "\n",
        "        LogTransformer : ['1stFlrSF','GrLivArea','LotArea', 'LotFrontage',]\n",
        "\n",
        "        PowerTransformer : ['2ndFlrSF', 'BsmtExposure', 'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF', 'GarageArea']\n",
        "\n",
        "        Winsorizer\" : ['GrLivArea'])),\n",
        "                                      \n",
        "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=None, method=\"spearman\", threshold=0.6, selection_method=\"variance\")),\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12 (default, Feb 16 2023, 12:40:47) \n[GCC 9.4.0]"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
